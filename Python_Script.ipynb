{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Rishit-dagli/Smart-Queuing-System-On-Edge/blob/master/Python_Script.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Create the Python Script\n",
    "\n",
    "In the cell below, you will need to complete the Python script and run the cell to generate the file using the magic `%%writefile` command. Your main task is to complete the following methods for the `PersonDetect` class:\n",
    "* `load_model`\n",
    "* `predict`\n",
    "* `draw_outputs`\n",
    "* `preprocess_outputs`\n",
    "* `preprocess_inputs`\n",
    "\n",
    "For your reference, here are all the arguments used for the argument parser in the command line:\n",
    "* `--model`:  The file path of the pre-trained IR model, which has been pre-processed using the model optimizer. There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware.\n",
    "* `--device`: The type of hardware you want to load the model on (CPU, GPU, MYRIAD, HETERO:FPGA,CPU)\n",
    "* `--video`: The file path of the input video.\n",
    "* `--output_path`: The location where the output stats and video file with inference needs to be stored (results/[device]).\n",
    "* `--max_people`: The max number of people in queue before directing a person to another queue.\n",
    "* `--threshold`: The probability threshold value for the person detection. Optional arg; default value is 0.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting person_detect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile person_detect.py\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "import os\n",
    "import cv2\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "class Queue:\n",
    "    \"\"\"\n",
    "    Class for dealing with queues.\n",
    "    \n",
    "    Performs basic operations for queues like adding to a queue, getting the queues \n",
    "    and checking the coordinates for queues.\n",
    "    \n",
    "    Attributes:\n",
    "        queues: A list containing the queues data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.queues=[]\n",
    "\n",
    "    def add_queue(self, points):\n",
    "        \"\"\"\n",
    "        Add points to the queue.\n",
    "\n",
    "        Args:\n",
    "            points: A list of points to be added.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: points is None.\n",
    "        \"\"\"\n",
    "        \n",
    "        if points is None:\n",
    "            raise TypeError\n",
    "        self.queues.append(points)\n",
    "\n",
    "    def get_queues(self, image):\n",
    "        \"\"\"\n",
    "        Get queues from images.\n",
    "\n",
    "        Args:\n",
    "            image: A list of the image.\n",
    "\n",
    "        Yields:\n",
    "            A list containing each frame.\n",
    "        \"\"\"\n",
    "    \n",
    "        for q in self.queues:\n",
    "            x_min, y_min, x_max, y_max=q\n",
    "            frame = image[y_min:y_max, x_min:x_max]\n",
    "            yield frame\n",
    "    \n",
    "    def check_coords(self, coords):\n",
    "        \"\"\"\n",
    "        Check coordinates for queues.\n",
    "\n",
    "        Args:\n",
    "            coords: A list of the coordinates.\n",
    "        \"\"\"\n",
    "    \n",
    "        d={k+1:0 for k in range(len(self.queues))}\n",
    "        for coord in coords:\n",
    "            for i, q in enumerate(self.queues):\n",
    "                if coord[0]>q[0] and coord[2]<q[2]:\n",
    "                    d[i+1]+=1\n",
    "        return d\n",
    "\n",
    "\n",
    "class PersonDetect:\n",
    "    \"\"\"\n",
    "    Class for the Person Detection Model.\n",
    "    \n",
    "    Performs person detection and preprocessing.\n",
    "    \n",
    "    Attributes:\n",
    "        model_weights: A string containing model weights path.\n",
    "        model_structure: A string conatining model structure path.\n",
    "        device: A string conatining device name.\n",
    "        threshold: A floating point number containing threshold value.\n",
    "        input_name: A list of input names.\n",
    "        input_shape: A tuple of the input shape.\n",
    "        output_name: A list of output names.\n",
    "        output_shape: A tuple of the output shape.\n",
    "        core: IECore object.\n",
    "        net: Loaded net object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, device, threshold=0.60):\n",
    "        \"\"\"\n",
    "        Inits PersonDetect class with model_name, device, threshold.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model_weights=model_name+'.bin'\n",
    "        self.model_structure=model_name+'.xml'\n",
    "        self.device=device\n",
    "        self.threshold=threshold\n",
    "\n",
    "        try:\n",
    "            self.model=IENetwork(self.model_structure, self.model_weights)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Could not Initialise the network. Have you enterred the correct model path?\")\n",
    "\n",
    "        self.input_name=next(iter(self.model.inputs))\n",
    "        self.input_shape=self.model.inputs[self.input_name].shape\n",
    "        self.output_name=next(iter(self.model.outputs))\n",
    "        self.output_shape=self.model.outputs[self.output_name].shape\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the model.\n",
    "        \"\"\"\n",
    "    \n",
    "        self.core = IECore()\n",
    "        self.net = self.core.load_network(network=self.model, device_name=self.device, num_requests=1)\n",
    "        \n",
    "    def predict(self, image):\n",
    "        \"\"\"\n",
    "        Make asynchronous predictions from images.\n",
    "\n",
    "        Args:\n",
    "            image: List of the image data.\n",
    "\n",
    "        Returns:\n",
    "            The outputs and the image.\n",
    "        \"\"\"\n",
    "       \n",
    "        input_name = self.input_name\n",
    "\n",
    "        input_img = self.preprocess_input(image)\n",
    "        input_dict={input_name: input_img}  \n",
    "        \n",
    "        infer_request_handle = self.net.start_async(request_id=0, inputs=input_dict)\n",
    "        infer_status = infer_request_handle.wait()\n",
    "        if infer_status == 0:\n",
    "            outputs = infer_request_handle.outputs[self.output_name]\n",
    "            \n",
    "        return outputs, image\n",
    "    \n",
    "    def draw_outputs(self, coords, image):\n",
    "        \"\"\"\n",
    "        Draws outputs or predictions on image.\n",
    "\n",
    "        Args:\n",
    "            coords: The coordinates of predictions.\n",
    "            image: The image on which boxes need to be drawn.\n",
    "\n",
    "        Returns:\n",
    "            the frame\n",
    "            the count of people\n",
    "            bounding boxes above threshold\n",
    "        \"\"\"\n",
    "    \n",
    "        current_count = 0\n",
    "        det = []        \n",
    "        for obj in coords[0][0]:\n",
    "            if obj[2] > self.threshold:\n",
    "                xmin = int(obj[3] * initial_w)\n",
    "                ymin = int(obj[4] * initial_h)\n",
    "                xmax = int(obj[5] * initial_w)\n",
    "                ymax = int(obj[6] * initial_h)\n",
    "                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 55, 255), 1)\n",
    "                current_count = current_count + 1\n",
    "                \n",
    "                det.append(obj)\n",
    "                \n",
    "        return frame, current_count, det\n",
    "\n",
    "    def preprocess_outputs(self, outputs):\n",
    "        \"\"\"\n",
    "        Preprocess the outputs.\n",
    "\n",
    "        Args:\n",
    "            outputs: The output from predictions.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed dictionary.\n",
    "        \"\"\"\n",
    "    \n",
    "        output_dict = {}\n",
    "        for output in outputs:\n",
    "            output_name = self.output_name\n",
    "            output_img = output\n",
    "            output_dict[output_name] = output_img\n",
    "        \n",
    "        return output_dict\n",
    "        return output\n",
    "        \n",
    "    def preprocess_input(self, image):\n",
    "\n",
    "        input_img = image\n",
    "               \n",
    "        n, c, h, w = self.input_shape\n",
    "        \n",
    "        input_img=cv2.resize(input_img, (w, h), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        # Change image from HWC to CHW\n",
    "        input_img = input_img.transpose((2, 0, 1))\n",
    "        input_img = input_img.reshape((n, c, h, w))\n",
    "\n",
    "        return input_img\n",
    "\n",
    "def main(args):\n",
    "    model=args.model\n",
    "    device=args.device\n",
    "    video_file=args.video\n",
    "    max_people=args.max_people\n",
    "    threshold=args.threshold\n",
    "    output_path=args.output_path\n",
    "\n",
    "    start_model_load_time=time.time()\n",
    "    pd= PersonDetect(model, device, threshold)\n",
    "    pd.load_model()\n",
    "    total_model_load_time = time.time() - start_model_load_time\n",
    "\n",
    "    queue=Queue()\n",
    "    \n",
    "    try:\n",
    "        queue_param=np.load(args.queue_param)\n",
    "        for q in queue_param:\n",
    "            queue.add_queue(q)\n",
    "    except:\n",
    "        print(\"error loading queue param file\")\n",
    "\n",
    "    try:\n",
    "        cap=cv2.VideoCapture(video_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cannot locate video file: \"+ video_file)\n",
    "    except Exception as e:\n",
    "        print(\"Something else went wrong with the video file: \", e)\n",
    "    \n",
    "    initial_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    initial_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    video_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    out_video = cv2.VideoWriter(os.path.join(output_path, 'output_video.mp4'), cv2.VideoWriter_fourcc(*'avc1'), fps, (initial_w, initial_h), True)\n",
    "    \n",
    "    counter=0\n",
    "    start_inference_time=time.time()\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame=cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            counter+=1\n",
    "            \n",
    "            coords, image= pd.predict(frame)\n",
    "            num_people= queue.check_coords(coords)\n",
    "            print(f\"Total People in frame = {len(coords)}\")\n",
    "            print(f\"Number of people in queue = {num_people}\")\n",
    "            out_text=\"\"\n",
    "            y_pixel=25\n",
    "            \n",
    "            for k, v in num_people.items():\n",
    "                out_text += f\"No. of People in Queue {k} is {v} \"\n",
    "                if v >= int(max_people):\n",
    "                    out_text += f\" Queue full; Please move to next Queue \"\n",
    "                cv2.putText(image, out_text, (15, y_pixel), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 255, 0), 2)\n",
    "                out_text=\"\"\n",
    "                y_pixel+=40\n",
    "            out_video.write(image)\n",
    "            \n",
    "        total_time=time.time()-start_inference_time\n",
    "        total_inference_time=round(total_time, 1)\n",
    "        fps=counter/total_inference_time\n",
    "\n",
    "        with open(os.path.join(output_path, 'stats.txt'), 'w') as f:\n",
    "            f.write(str(total_inference_time)+'\\n')\n",
    "            f.write(str(fps)+'\\n')\n",
    "            f.write(str(total_model_load_time)+'\\n')\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    except Exception as e:\n",
    "        print(\"Could not run Inference: \", e)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument('--model', required=True)\n",
    "    parser.add_argument('--device', default='CPU')\n",
    "    parser.add_argument('--video', default=None)\n",
    "    parser.add_argument('--queue_param', default=None)\n",
    "    parser.add_argument('--output_path', default='/results')\n",
    "    parser.add_argument('--max_people', default=2)\n",
    "    parser.add_argument('--threshold', default=0.60)\n",
    "    \n",
    "    args=parser.parse_args()\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step\n",
    "\n",
    "Now that you've run the above cell and created your Python script, you will create your job submission shell script in the next notebook."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
